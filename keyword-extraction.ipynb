{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3416a6fc",
   "metadata": {},
   "source": [
    "Type: Learning Project \\\n",
    "Author: Yash K\n",
    "\n",
    "#### About: Keyword Extraction\n",
    "Keyword extraction is defined as the task of Natural language processing that automatically identifies a set of terms to describe the subject of the text. This is an important method in information retrieval (IR) systems: keywords simplify and speed up research. Keyword extraction can be used to reduce text dimensionality for further text analysis (subject modeling text classification).\n",
    "\n",
    "Data set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f14578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# ^^^ pyforest auto-imports - don't write above this line\n",
    "#imports\n",
    "#generic/data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#utility\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#ML\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "\n",
    "#NLP\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75c2fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./keyword-extraction-data/papers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d788ddf",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956cada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year range:  1987 2017\n"
     ]
    }
   ],
   "source": [
    "print('year range: ', min(df['year']), max(df['year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a3c9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct event types: [nan 'Oral' 'Spotlight' 'Poster']\n"
     ]
    }
   ],
   "source": [
    "print('distinct event types:', df['event_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d40ae5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstracts present: 3922\n"
     ]
    }
   ],
   "source": [
    "print('abstracts present:', len(df['abstract'].unique())-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b277120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperTexts = df['abstract'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0910916c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover, the vast majority of currently available models are explicitly designed for capturing some specific graph properties (such as power-law degree distributions), which makes them unsuitable for application to domains where the behavior of the target quantities is not known a priori. The key contribution of this paper is twofold. First, we introduce the Fiedler delta statistic, based on the Laplacian spectrum of graphs, which allows to dispense with any parametric assumption concerning the modeled network properties. Second, we use the defined statistic to develop the Fiedler random field model, which allows for efficient estimation of edge distributions over large-scale random networks. After analyzing the dependence structure involved in Fiedler random fields, we estimate them over several real-world networks, showing that they achieve a much higher modeling accuracy than other well-known statistical approaches.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperTexts[1267]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a0218",
   "metadata": {},
   "source": [
    "### Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e437fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stop words for english\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba3e0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0028283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating a list of custom stopwords\n",
    "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n",
    "             \"show\", \"result\", \"large\", \n",
    "             \"also\", \"one\", \"two\", \"three\", \n",
    "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\n",
    "stop_words = list(stop_words.union(new_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9d53dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create preprocessor\n",
    "def pre_process(text):\n",
    "    text=text.lower() #lowercase\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text) #remove tags\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text) #remove digits and special characters = NOT(a-zA-Z0-9_) \n",
    "    text=text.split() #split text | Tokensize\n",
    "    text=[word for word in text if word not in stop_words] #remove stopwords\n",
    "    text=[word for word in text if len(word)>=3] #remove very small words\n",
    "    lmtzr=WordNetLemmatizer()\n",
    "    text=[lmtzr.lemmatize(word) for word in text] # Lemmatize\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed3c9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['paper_text'].apply(lambda x:pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "676b4cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       self organization associative database applica...\n",
       "1       mean field theory layer visual cortex applicat...\n",
       "2       storing covariance associative long term poten...\n",
       "3       bayesian query construction neural network mod...\n",
       "4       neural network ensemble cross validation activ...\n",
       "                              ...                        \n",
       "7236    single transistor learning synapsis paul hasle...\n",
       "7237    bias variance combination least square estimat...\n",
       "7238    real time clustering cmos neural engine serran...\n",
       "7239    learning direction global motion class psychop...\n",
       "7240    correlation interpolation network real time ex...\n",
       "Name: paper_text, Length: 7241, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac668d5a",
   "metadata": {},
   "source": [
    "### TF-IDF \n",
    "TF-IDF stands for Text Frequency Inverse Document Frequency. The importance of each word increases in proportion to the number of times a word appears in the document (Text Frequency – TF) but is offset by the frequency of the word in the corpus (Inverse Document Frequency – IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2743686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57340c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a vocabulary of words, \n",
    "cv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n",
    "                   max_features=10000,  # the size of the vocabulary\n",
    "                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n",
    "                  )\n",
    "word_count_matrix=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "550e2226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7241x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5934926 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_matrix # n_docs * vocab_size sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff487bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0ba8dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashkhandelwal/opt/anaconda3/envs/projectEnv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "feature_names=cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d02ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get keywords in the idx index document\n",
    "def get_keywords(idx, docs, n):\n",
    "    #generate tf_idf for the given doc\n",
    "    tf_idf_vec = tfidf_transformer.transform(cv.transform([docs[idx]]))\n",
    "    #sort tf_idf vectors by descending order of scores\n",
    "    sorted_items=sort_(tf_idf_vec.tocoo())\n",
    "    #get only top n and return \n",
    "    keywords=extract_topn(feature_names, sorted_items, n)\n",
    "    return keywords\n",
    "\n",
    "#sort the coordinate sparse matrix\n",
    "def sort_(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x:(x[1], x[0]), reverse=True)\n",
    "\n",
    "#extract and return top n results\n",
    "def extract_topn(feature_names, sorted_items, n):\n",
    "    result = {feature_names[idx]:round(score, 3) for idx, score in sorted_items[:n]}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1500954",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f1eb072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ica': 0.349, 'independent component': 0.343, 'component': 0.301, 'kurtosis': 0.245, 'multiplier': 0.239, 'lagrange multiplier': 0.239, 'signal': 0.227, 'lagrange': 0.226, 'independent': 0.15, 'matrix': 0.145, 'independent component analysis': 0.126, 'constrained': 0.12, 'resulted': 0.112, 'source': 0.108, 'constraint': 0.107, 'variance': 0.106, 'snr': 0.092, 'component analysis': 0.091, 'augmented lagrangian': 0.09, 'ordering': 0.089}\n"
     ]
    }
   ],
   "source": [
    "index = 1023\n",
    "topn=20\n",
    "keywords=get_keywords(index, docs, topn)\n",
    "print(keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectEnv",
   "language": "python",
   "name": "projectenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
